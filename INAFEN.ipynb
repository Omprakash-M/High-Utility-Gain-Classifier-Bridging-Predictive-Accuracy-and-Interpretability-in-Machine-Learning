{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8af072f-499e-409b-a4d5-6220957d42ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTEST\n",
      "             6       148        72        35         0      33.6     0.627  \\\n",
      "667  0.002852 -0.283659 -0.213733  0.067348  0.157877 -0.005814 -0.107625   \n",
      "324 -0.198030  0.068302 -0.096203 -0.193585  0.157877 -0.241838 -0.193148   \n",
      "623 -0.198030 -0.172758 -0.213733 -0.060693 -0.072905 -0.005814 -0.193148   \n",
      "689  0.189738 -0.172758 -0.004482 -0.060693 -0.072905 -0.302006  0.024376   \n",
      "521  0.002852 -0.172758 -0.213733 -0.060693 -0.072905 -0.302006 -0.193148   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "355 -0.198030  0.068302 -0.213733  0.067348  0.157877 -0.005814  0.246603   \n",
      "533 -0.198030 -0.283659 -0.213733 -0.193585 -0.219067 -0.005814  0.246603   \n",
      "344  0.189738  0.068302  0.178566  0.067348 -0.219067 -0.005814 -0.107625   \n",
      "296 -0.198030  0.068302  0.178566 -0.193585  0.157877 -0.005814 -0.107625   \n",
      "720 -0.198030 -0.172758 -0.096203  0.067348  0.157877 -0.005814 -0.107625   \n",
      "\n",
      "           50  ['148-[[-inf, -0.6683599948883057]]']  \n",
      "667  0.120258                                      1  \n",
      "324 -0.191597                                      0  \n",
      "623 -0.273695                                      0  \n",
      "689  0.120258                                      0  \n",
      "521 -0.191597                                      0  \n",
      "..        ...                                    ...  \n",
      "355  0.120258                                      0  \n",
      "533 -0.191597                                      1  \n",
      "344  0.120258                                      0  \n",
      "296 -0.191597                                      0  \n",
      "720 -0.273695                                      0  \n",
      "\n",
      "[154 rows x 9 columns]\n",
      "\n",
      " Evaluation Metrics for INAFEN:\n",
      "Accuracy     : 0.7532\n",
      "F1 Score     : 0.6481\n",
      "AUC Score    : 0.8442\n",
      "Log Loss     : 0.4873\n",
      "H-measure    : 0.4485363779765409\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, log_loss\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "from INAFEN import InterpretableAutomatedFeatureEngineering\n",
    "\n",
    "# Optional: H-measure (install via pip if you want it)\n",
    "try:\n",
    "    from hmeasure import h_score\n",
    "    hmeasure_available = True\n",
    "except ImportError:\n",
    "    hmeasure_available = False\n",
    "    \n",
    "\n",
    "# Load dataset\n",
    "data_df = pd.read_csv('datasets/pima indians diabetes.csv')\n",
    "\n",
    "# Preprocessing\n",
    "df_feas = data_df.iloc[:, :-1]\n",
    "df_target = data_df.iloc[:, -1]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_feas_scaled = pd.DataFrame(scaler.fit_transform(df_feas), columns=df_feas.columns)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_target_encoded = pd.Series(label_encoder.fit_transform(df_target), name='classes')\n",
    "\n",
    "df_preprocessed = pd.concat([df_feas_scaled, df_target_encoded], axis=1)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_feas_scaled, df_target_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train teacher model\n",
    "teacher_model = XGBClassifier(n_estimators=100, max_depth=3, gamma=1, eval_metric='logloss')\n",
    "teacher_model.fit(X_train, y_train)\n",
    "\n",
    "# Prepare INAFEN\n",
    "num_categorical_feas = 0\n",
    "num_numerical_feas = X_train.shape[1]\n",
    "\n",
    "INAFEN_model = InterpretableAutomatedFeatureEngineering(\n",
    "    DTFT=True,\n",
    "    ARFC=True,\n",
    "    BMKD=True,\n",
    "    number_of_categorical_features=num_categorical_feas,\n",
    "    number_of_numerical_features=num_numerical_feas,\n",
    "    FC_min_support=0.2,\n",
    "    FC_min_confidence=0.85,\n",
    "    ST_temperature=1,\n",
    "    ST_alpha=1\n",
    ")\n",
    "\n",
    "# Fit INAFEN\n",
    "INAFEN_model.fit(\n",
    "    X=pd.DataFrame(X_train, columns=df_feas.columns),\n",
    "    y_true=pd.DataFrame(y_train, columns=['classes']),\n",
    "    y_soft=teacher_model.predict_proba(X_train)[:, 1]\n",
    ")\n",
    "\n",
    "# Predict & evaluate\n",
    "X_test_transformed = INAFEN_model.transform(pd.DataFrame(X_test, columns=df_feas.columns))\n",
    "y_pred = INAFEN_model.predict(X_test_transformed)\n",
    "y_pred_proba = INAFEN_model.predict_proba(X_test_transformed)[:, 1]\n",
    "# Metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "lloss = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "if hmeasure_available:\n",
    "    h = h_score(y_test.values, y_pred_proba)\n",
    "else:\n",
    "    h = 'Unavailable'\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n Evaluation Metrics for INAFEN:\")\n",
    "print(f\"Accuracy     : {acc:.4f}\")\n",
    "print(f\"F1 Score     : {f1:.4f}\")\n",
    "print(f\"AUC Score    : {auc:.4f}\")\n",
    "print(f\"Log Loss     : {lloss:.4f}\")\n",
    "print(f\"H-measure    : {h if h != 'Unavailable' else 'Install hmeasure for this'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd322513-87d2-4f53-a8c2-0b61c4ef1dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy:   0.5205\n",
      "Final AUC:        0.5000\n",
      "Final F1 Score:   0.0000\n",
      "Final Log Loss:   0.6925\n",
      "Final H-measure:  0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, log_loss\n",
    "from xgboost import XGBClassifier\n",
    "from INAFEN import InterpretableAutomatedFeatureEngineering\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Step 1: Load and preprocess raw data ---\n",
    "dataset_path = 'datasets/heloc.csv'\n",
    "data_df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Filter out rows with 20 or more -9s\n",
    "data_df = data_df.loc[(data_df == -9).sum(axis=1) < 20].reset_index(drop=True)\n",
    "\n",
    "# Replace coded missing values with np.nan\n",
    "data_df = data_df.replace([-7, -8, -9], np.nan)\n",
    "\n",
    "# Feature columns (excluding target)\n",
    "feature_cols = data_df.columns[1:]\n",
    "mean_imputing_cols = feature_cols.tolist()\n",
    "\n",
    "# Mean imputation\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "data_df[mean_imputing_cols] = mean_imputer.fit_transform(data_df[mean_imputing_cols])\n",
    "\n",
    "# Separate features and target\n",
    "df_feas = data_df.iloc[:, 1:]\n",
    "df_target = data_df.iloc[:, 0]\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "df_target = pd.DataFrame(label_encoder.fit_transform(df_target), columns=['classes'])\n",
    "\n",
    "# Combine into full dataframe\n",
    "df_preprocessed = pd.concat([df_feas, df_target], axis=1)\n",
    "X = df_feas.values\n",
    "y = df_target.values.ravel()\n",
    "df_preprocessed_columns = df_feas.columns\n",
    "\n",
    "# --- Step 2: Prepare for INAFEN training ---\n",
    "num_categorical_feas = 0\n",
    "num_numerical_feas = len(df_feas.columns)\n",
    "\n",
    "args = {\n",
    "    'DTFT': True,\n",
    "    'ARFC': True,\n",
    "    'BMKD': True,\n",
    "    'TBD_max_depth_dict': None,\n",
    "    'FC_min_support': 0.2,\n",
    "    'FC_min_confidence': 0.85,\n",
    "    'ST_temperature': 1,\n",
    "    'ST_alpha': 1\n",
    "}\n",
    "\n",
    "# --- Step 3: Train/Test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train a teacher model for soft labels\n",
    "teacher_model = XGBClassifier(n_estimators=100, max_depth=3, gamma=1, use_label_encoder=False, eval_metric='logloss')\n",
    "teacher_model.fit(X_train, y_train)\n",
    "\n",
    "# Initialize and train INAFEN\n",
    "INAFEN_model = InterpretableAutomatedFeatureEngineering(\n",
    "    DTFT=args['DTFT'],\n",
    "    ARFC=args['ARFC'],\n",
    "    BMKD=args['BMKD'],\n",
    "    number_of_categorical_features=num_categorical_feas,\n",
    "    number_of_numerical_features=num_numerical_feas,\n",
    "    TBD_max_depth_dict=args['TBD_max_depth_dict'],\n",
    "    FC_min_support=args['FC_min_support'],\n",
    "    FC_min_confidence=args['FC_min_confidence'],\n",
    "    ST_temperature=args['ST_temperature'],\n",
    "    ST_alpha=args['ST_alpha']\n",
    ")\n",
    "\n",
    "INAFEN_model.fit(\n",
    "    X=pd.DataFrame(X_train, columns=df_preprocessed_columns),\n",
    "    y_true=pd.DataFrame(y_train, columns=['classes']),\n",
    "    y_soft=teacher_model.predict_proba(X_train)[:, 1]\n",
    ")\n",
    "\n",
    "# --- Step 4: Evaluate ---\n",
    "X_test_transformed = INAFEN_model.transform(pd.DataFrame(X_test, columns=df_preprocessed_columns))\n",
    "y_pred_proba = INAFEN_model.predict_proba(X_test_transformed)[:, 1]\n",
    "y_pred = INAFEN_model.predict(X_test_transformed)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# AUC\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Log Loss\n",
    "lloss = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "# H-measure (try using optional hmeasure module, otherwise skip)\n",
    "try:\n",
    "    from hmeasure import h_score\n",
    "    h_measure = h_score(y_test, y_pred_proba)\n",
    "except Exception as e:\n",
    "    h_measure = f\"H-measure error: {e}\"\n",
    "\n",
    "except ImportError:\n",
    "    h_measure = \"H-measure module not installed\"\n",
    "\n",
    "# --- Print all metrics ---\n",
    "print(f\"Final Accuracy:   {accuracy:.4f}\")\n",
    "print(f\"Final AUC:        {auc:.4f}\")\n",
    "print(f\"Final F1 Score:   {f1:.4f}\")\n",
    "print(f\"Final Log Loss:   {lloss:.4f}\")\n",
    "print(f\"Final H-measure:  {h_measure}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b62f8af1-8981-4d93-bbb2-1efe0c6e6999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC:  0.9003, Accuracy: 0.9011, F1: 0.3826, LogLoss: 0.2662, H: 0.5166960285428823\n",
      "Test AUC:        0.9100, Accuracy: 0.9073, F1: 0.4203, LogLoss: 0.2481, H: 0.5529728350587362\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score, log_loss, roc_auc_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from INAFEN import InterpretableAutomatedFeatureEngineering\n",
    "\n",
    "# --- Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Load data\n",
    "data_path = 'datasets/'\n",
    "dataset_name = 'bank marketing.csv'\n",
    "data_df = pd.read_csv(os.path.join(data_path, dataset_name), delimiter=';')\n",
    "\n",
    "# --- Clean column names (in case of trailing spaces)\n",
    "data_df.columns = data_df.columns.str.strip()\n",
    "\n",
    "# --- Data preprocessing\n",
    "df_target = data_df['y']\n",
    "df_feas = data_df.drop(columns=['y'])\n",
    "\n",
    "categorical_cols = df_feas.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = df_feas.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "num_categorical_feas = len(categorical_cols)\n",
    "num_numerical_feas = len(numerical_cols)\n",
    "\n",
    "df_categorical_encoded = pd.get_dummies(df_feas[categorical_cols], drop_first=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_numerical_scaled = pd.DataFrame(scaler.fit_transform(df_feas[numerical_cols]), columns=numerical_cols)\n",
    "\n",
    "df_feas_processed = pd.concat([df_numerical_scaled, df_categorical_encoded], axis=1)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_target_encoded = pd.DataFrame(label_encoder.fit_transform(df_target), columns=['classes'])\n",
    "\n",
    "# --- Final dataset\n",
    "X = df_feas_processed.values\n",
    "y = df_target_encoded.values.ravel()\n",
    "data_df_columns = df_feas_processed.columns\n",
    "\n",
    "# --- Split once into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# --- Further split for teacher training and validation\n",
    "X_train_train, X_train_valid, y_train_train, y_train_valid = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "\n",
    "# --- Train teacher model\n",
    "teacher_model = XGBClassifier(n_estimators=100, max_depth=3, gamma=1, random_state=42)\n",
    "teacher_model.fit(X_train_train, y_train_train)\n",
    "\n",
    "# --- Train INAFEN\n",
    "parameter_dict = {\n",
    "    'FC_min_support': 0.2,\n",
    "    'FC_min_confidence': 0.85\n",
    "}\n",
    "\n",
    "model = InterpretableAutomatedFeatureEngineering(\n",
    "    DTFT=True,\n",
    "    ARFC=True,\n",
    "    BMKD=True,\n",
    "    number_of_categorical_features=num_categorical_feas,\n",
    "    number_of_numerical_features=num_numerical_feas,\n",
    "    TBD_max_depth_dict=None,\n",
    "    **parameter_dict\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X=pd.DataFrame(X_train_train, columns=data_df_columns),\n",
    "    y_true=pd.DataFrame(y_train_train, columns=['classes']),\n",
    "    y_soft=teacher_model.predict_proba(X_train_train)[:, 1]\n",
    ")\n",
    "\n",
    "# --- Transform and Evaluate\n",
    "X_val_trans = model.transform(pd.DataFrame(X_train_valid, columns=data_df_columns))\n",
    "X_test_trans = model.transform(pd.DataFrame(X_test, columns=data_df_columns))\n",
    "\n",
    "val_preds = model.predict_proba(X_val_trans)[:, 1]\n",
    "test_preds = model.predict_proba(X_test_trans)[:, 1]\n",
    "\n",
    "val_auc = roc_auc_score(y_train_valid, val_preds)\n",
    "test_auc = roc_auc_score(y_test, test_preds)\n",
    "\n",
    "val_f1 = f1_score(y_train_valid, np.round(val_preds))\n",
    "test_f1 = f1_score(y_test, np.round(test_preds))\n",
    "\n",
    "val_logloss = log_loss(y_train_valid, val_preds)\n",
    "test_logloss = log_loss(y_test, test_preds)\n",
    "\n",
    "val_acc = accuracy_score(y_train_valid, np.round(val_preds))\n",
    "test_acc = accuracy_score(y_test, np.round(test_preds))\n",
    "\n",
    "try:\n",
    "    from hmeasure import h_score\n",
    "    val_h = h_score(y_train_valid, val_preds)\n",
    "    test_h = h_score(y_test, test_preds)\n",
    "except ImportError:\n",
    "    val_h = test_h = \"hmeasure module not installed\"\n",
    "\n",
    "# --- Print results\n",
    "print(f\"Validation AUC:  {val_auc:.4f}, Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}, LogLoss: {val_logloss:.4f}, H: {val_h}\")\n",
    "print(f\"Test AUC:        {test_auc:.4f}, Accuracy: {test_acc:.4f}, F1: {test_f1:.4f}, LogLoss: {test_logloss:.4f}, H: {test_h}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b4c81d-75e9-445e-af33-6fdbf40f101f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
